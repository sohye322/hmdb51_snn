{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyPhuIawd45EsYRmOBRSwID/"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XgP-f437SSPl","executionInfo":{"status":"ok","timestamp":1741439739368,"user_tz":-540,"elapsed":1738,"user":{"displayName":"반도체시스템공학과/양소혜","userId":"15927555157164829597"}},"outputId":"bba0e154-ec92-40ee-b034-ca1c3c21e428"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","import os\n","\n","drive.mount('/content/drive')\n","os.listdir(\"/content/drive/MyDrive\")\n","os.chdir(\"/content/drive/MyDrive/hmdb51\")"]},{"cell_type":"markdown","source":["# Load HMDB51"],"metadata":{"id":"R6wSnKNS22IS"}},{"cell_type":"markdown","source":["Before running below code, download HMDB51 first."],"metadata":{"id":"lF5H-c9a6bwI"}},{"cell_type":"code","source":["! pip install av\n","! pip install spikingjelly\n","#! wget https://raw.githubusercontent.com/pytorch/vision/6de158c473b83cf43344a0651d7c01128c7850e6/references/video_classification/transforms.py"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jJIGG2m1ShZ_","executionInfo":{"status":"ok","timestamp":1741439749529,"user_tz":-540,"elapsed":8022,"user":{"displayName":"반도체시스템공학과/양소혜","userId":"15927555157164829597"}},"outputId":"66bfda6c-862b-410e-9538-2ad170f2794b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: av in /usr/local/lib/python3.11/dist-packages (14.2.0)\n","Requirement already satisfied: spikingjelly in /usr/local/lib/python3.11/dist-packages (0.0.0.0.14)\n","Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from spikingjelly) (2.5.1+cu121)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from spikingjelly) (3.10.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from spikingjelly) (1.26.4)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from spikingjelly) (4.67.1)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from spikingjelly) (0.20.1+cu121)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from spikingjelly) (1.13.1)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->spikingjelly) (1.3.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->spikingjelly) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->spikingjelly) (4.55.6)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->spikingjelly) (1.4.8)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->spikingjelly) (24.2)\n","Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->spikingjelly) (11.1.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->spikingjelly) (3.2.1)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->spikingjelly) (2.8.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->spikingjelly) (3.17.0)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch->spikingjelly) (4.12.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->spikingjelly) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->spikingjelly) (3.1.5)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->spikingjelly) (2024.10.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch->spikingjelly) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch->spikingjelly) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch->spikingjelly) (12.1.105)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->spikingjelly) (9.1.0.70)\n","Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch->spikingjelly) (12.1.3.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch->spikingjelly) (11.0.2.54)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch->spikingjelly) (10.3.2.106)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch->spikingjelly) (11.4.5.107)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch->spikingjelly) (12.1.0.106)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->spikingjelly) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch->spikingjelly) (12.1.105)\n","Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch->spikingjelly) (3.1.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->spikingjelly) (1.13.1)\n","Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->spikingjelly) (12.8.61)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->spikingjelly) (1.3.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->spikingjelly) (1.17.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->spikingjelly) (3.0.2)\n"]}]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","from torch.nn import functional as F\n","from torch.utils.data import random_split, DataLoader\n","from torch.optim.lr_scheduler import StepLR\n","import torchvision\n","from torchvision import get_video_backend\n","from torchvision.models.video import r3d_18\n","from torchvision import transforms\n","import os\n","from tqdm.auto import tqdm\n","import numpy as np\n","import time\n","import datetime\n","import random\n","import transforms as T\n","import av\n","\n","from spikingjelly.activation_based import layer, neuron, surrogate, encoding, functional"],"metadata":{"id":"LLayrsggSjtv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Datasets and Dataloaders for model training ..\n","\n","val_split = 0.05 # 검증 데이터 비율 (5%)\n","num_frames = 16\n","clip_steps = 50 # 연속된 프레임 사이의 간격\n","num_workers = 8\n","pin_memory = True # GPU 사용 시 데이터 로딩 성능 최적화\n","train_tfms = torchvision.transforms.Compose([\n","                                 T.ToFloatTensorInZeroOne(), # 0~1 범위의 float32 tensor\n","                                 T.Resize((128, 171)), # 128x171\n","                                 T.RandomHorizontalFlip(),# 랜덤으로 좌우 반전\n","                                 T.Normalize(mean=[0.43216, 0.394666, 0.37645], std=[0.22803, 0.22145, 0.216989]),\n","                                 T.RandomCrop((112, 112)) # 112x112\n","                               ])\n","test_tfms =  torchvision.transforms.Compose([\n","                                             T.ToFloatTensorInZeroOne(),\n","                                             T.Resize((128, 171)),\n","                                             T.Normalize(mean=[0.43216, 0.394666, 0.37645], std=[0.22803, 0.22145, 0.216989]),\n","                                             T.CenterCrop((112, 112))\n","                                             ])\n","hmdb51_train = torchvision.datasets.HMDB51('video_data/', 'test_train_splits/', num_frames,\n","                                                step_between_clips = clip_steps, fold=1, train=True,\n","                                                transform=train_tfms, num_workers=num_workers)\n","\n","\n","hmdb51_test = torchvision.datasets.HMDB51('video_data/', 'test_train_splits/', num_frames,\n","                                                step_between_clips = clip_steps, fold=1, train=False,\n","                                                transform=test_tfms, num_workers=num_workers)\n","\n","total_train_samples = len(hmdb51_train)\n","total_val_samples = round(val_split * total_train_samples)\n","\n","print(f\"number of train samples {total_train_samples}\")\n","print(f\"number of validation samples {total_val_samples}\")\n","print(f\"number of test samples {len(hmdb51_test)}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ffLdBHrAztPb","executionInfo":{"status":"ok","timestamp":1741425839620,"user_tz":-540,"elapsed":586192,"user":{"displayName":"반도체시스템공학과/양소혜","userId":"15927555157164829597"}},"outputId":"1bebfce3-f33d-46dd-8e49-78ae78fdd2ef"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(\n","100%|██████████| 417/417 [07:45<00:00,  1.12s/it]\n","100%|██████████| 417/417 [01:57<00:00,  3.55it/s]\n"]},{"output_type":"stream","name":"stdout","text":["number of train samples 7577\n","number of validation samples 379\n","number of test samples 3161\n"]}]},{"cell_type":"code","source":["batch_size = 16\n","num_workers = 0\n","\n","kwargs = {'num_workers':num_workers, 'pin_memory':True} if torch.cuda.is_available() else {'num_workers':num_workers}\n","#kwargs = {'num_workers':num_workers}\n","#kwargs = {}\n","\n","hmdb51_train_v1, hmdb51_val_v1 = random_split(hmdb51_train, [total_train_samples - total_val_samples,\n","                                                                       total_val_samples])\n","\n","#hmdb51_train_v1.video_clips.compute_clips(16, 1, frame_rate=30)\n","#hmdb51_val_v1.video_clips.compute_clips(16, 1, frame_rate=30)\n","#hmdb51_test.video_clips.compute_clips(16, 1, frame_rate=30)\n","\n","#train_sampler = RandomClipSampler(hmdb51_train_v1.video_clips, 5)\n","#test_sampler = UniformClipSampler(hmdb51_test.video_clips, 5)\n","\n","train_loader = DataLoader(hmdb51_train_v1, batch_size=batch_size, shuffle=True, **kwargs)\n","val_loader   = DataLoader(hmdb51_val_v1, batch_size=batch_size, shuffle=True, **kwargs)\n","test_loader  = DataLoader(hmdb51_test, batch_size=batch_size, shuffle=False, **kwargs)"],"metadata":{"id":"6RjPNYUTzxcW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["batch = next(iter(train_loader))\n","print(f\"Batch type: {type(batch)}\")\n","print(f\"Batch length: {len(batch)}\")\n","\n","video, audio, label = next(iter(train_loader))\n","print(video.shape) # (batch size, channels, frames, height, width)\n","print(audio.shape)\n","print(label.shape) # (batch size)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"J_CFwGKjzz7m","executionInfo":{"status":"ok","timestamp":1741426161341,"user_tz":-540,"elapsed":3238,"user":{"displayName":"반도체시스템공학과/양소혜","userId":"15927555157164829597"}},"outputId":"ff4a7ac2-d214-446e-ba0e-ebbddcb827f5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Batch type: <class 'list'>\n","Batch length: 3\n","torch.Size([16, 3, 16, 112, 112])\n","torch.Size([16, 1, 0])\n","torch.Size([16])\n"]}]},{"cell_type":"code","source":["import joblib\n","\n","# 데이터 로더 저장\n","joblib.dump(train_loader, \"hmdb51_train.pkl\")\n","joblib.dump(test_loader, \"hmdb51_test.pkl\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"umXjJNqF3KzF","executionInfo":{"status":"ok","timestamp":1741426163613,"user_tz":-540,"elapsed":251,"user":{"displayName":"반도체시스템공학과/양소혜","userId":"15927555157164829597"}},"outputId":"af498c5a-7bb9-43a4-ae21-4acc65d128ff"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['hmdb51_test.pkl']"]},"metadata":{},"execution_count":40}]},{"cell_type":"markdown","source":["앞으로는 이것만 하면 됨"],"metadata":{"id":"wuXlMdLa3O2T"}},{"cell_type":"code","source":["import joblib\n","\n","# 저장된 데이터 로더 불러오기\n","train_loader = joblib.load(\"hmdb51_train.pkl\")\n","test_loader = joblib.load(\"hmdb51_test.pkl\")"],"metadata":{"id":"aE4nEk_J3M3s"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(f'Number of batches in train_loader: {len(train_loader)}')\n","print(f'num_workers: {train_loader.num_workers}')\n","print(f'Batch size: {train_loader.batch_size}')\n","\n","video, audio, label = next(iter(train_loader))\n","print(f\"Video shape: {video.shape}\")  # (batch_size, frames, height, width)\n","print(f\"Audio shape: {audio.shape}\")  # 오디오 데이터 크기\n","print(f\"Label shape: {label.shape}\")  # (batch_size,)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QPw4YEgRAfUs","executionInfo":{"status":"ok","timestamp":1741439818809,"user_tz":-540,"elapsed":18048,"user":{"displayName":"반도체시스템공학과/양소혜","userId":"15927555157164829597"}},"outputId":"058a36ae-3b83-4604-8fee-98831935f3b5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of batches in train_loader: 450\n","num_workers: 0\n","Batch size: 16\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/torchvision/io/video.py:169: UserWarning: The pts_unit 'pts' gives wrong results. Please use pts_unit 'sec'.\n","  warnings.warn(\"The pts_unit 'pts' gives wrong results. Please use pts_unit 'sec'.\")\n"]},{"output_type":"stream","name":"stdout","text":["Video shape: torch.Size([16, 3, 16, 112, 112])\n","Audio shape: torch.Size([16, 1, 0])\n","Label shape: torch.Size([16])\n"]}]},{"cell_type":"markdown","source":["# ANN"],"metadata":{"id":"_ARXR9Tb3RCG"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","from torch.nn import functional as F\n","import torch.optim as optim\n","import time\n","import matplotlib.pyplot as plt"],"metadata":{"id":"8djXurPd5R1F"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class ANN(nn.Module):\n","    def __init__(self, input_size, output_size):\n","        super(ANN, self).__init__()\n","        self.network = nn.Sequential(\n","            nn.Flatten(),\n","            nn.Linear(input_size, output_size),\n","            nn.ReLU() # 출력값은 항상 0 이상, 출력 크기는 (batch_size, output_size)=(16,51)\n","        )\n","\n","    def forward(self, x):\n","        x = self.network(x)\n","        return x\n","\n","'''\n","class SNN(nn.Module):\n","    def __init__(self, input_size, output_size, tau=10.0, v_th=1.0, dt=1.0):\n","        super(SNN, self).__init__()\n","        self.weights = None\n","        self.lif = neuron.LIFNode(tau=tau, v_threshold=v_th, detach_reset=True)\n","\n","    def set_weights(self, weights, photo_responsivity):\n","        self.weights = weights * (max(abs(photo_responsivity)) / max(abs(weights)))\n","\n","    def forward(self, x):\n","        if self.weights is None:\n","          raise valueError(\"Weights have not been set. Use set_weights() first.\")\n","\n","        membrane_potential = torch.matmul(input_spikes, self.weights)\n","        return self.lif(membrane_potential)\n","'''"],"metadata":{"id":"sQA1QBNLSlVA","executionInfo":{"status":"ok","timestamp":1741439842529,"user_tz":-540,"elapsed":7,"user":{"displayName":"반도체시스템공학과/양소혜","userId":"15927555157164829597"}},"colab":{"base_uri":"https://localhost:8080/","height":162},"outputId":"22a8e0ca-a319-44e7-d0e9-6164121bf3fb"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\nclass SNN(nn.Module):\\n    def __init__(self, input_size, output_size, tau=10.0, v_th=1.0, dt=1.0):\\n        super(SNN, self).__init__()\\n        self.weights = None\\n        self.lif = neuron.LIFNode(tau=tau, v_threshold=v_th, detach_reset=True)\\n\\n    def set_weights(self, weights, photo_responsivity):\\n        self.weights = weights * (max(abs(photo_responsivity)) / max(abs(weights)))\\n\\n    def forward(self, x):\\n        if self.weights is None:\\n          raise valueError(\"Weights have not been set. Use set_weights() first.\")\\n\\n        membrane_potential = torch.matmul(input_spikes, self.weights)\\n        return self.lif(membrane_potential)\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":8}]},{"cell_type":"markdown","source":["**Train**"],"metadata":{"id":"RRcfv2Qj8GSO"}},{"cell_type":"code","source":["device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","input_size = 3 * 16 * 112 * 112\n","output_size = 51\n","ann = ANN(input_size, output_size).to(device)\n","\n","num_epochs = 100\n","learning_rate = 0.001\n","criterion = nn.MSELoss()\n","optimizer = optim.SGD(ann.parameters(), lr=learning_rate)"],"metadata":{"id":"SD6eRmKVl5w5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["max_test_acc = -1\n","train_acc_list = []\n","test_acc_list = []\n","\n","for epoch in range(num_epochs):\n","  start_time = time.time()\n","  ann.train()\n","  train_loss = 0\n","  train_acc = 0\n","  train_samples = 0\n","  for frame, _, label in train_loader:\n","    optimizer.zero_grad()\n","    frame = frame.to(device).permute(2,0,1,3,4) # [N,T,C,H,W] -> [T,N,C,H,W]\n","    label = label.to(device)\n","    label_onehot = F.one_hot(label,  51).float()\n","\n","    out_fr = ann(frame) # 크기: (batch_size, output_size)=(16,51)\n","    loss = F.mse_loss(out_fr, label_onehot)\n","    loss.backward()\n","    optimizer.step()\n","\n","    train_samples += label.numel()\n","    train_loss += loss.item() * label.numel()\n","    train_acc += (out_fr.argmax(1) == label).float().sum().item()\n","  train_time = time.time()\n","  train_speed = train_samples / (train_time - start_time)\n","  train_loss /= train_samples\n","  train_acc /= train_samples\n","  train_acc_list.append(train_acc)\n","  print(f'epoch {epoch}: train_loss={train_loss: .4f}, train_acc={train_acc: .4f}, train_speed={train_speed: .4f}')\n","\n","  ann.eval()\n","  test_loss = 0\n","  test_acc = 0\n","  test_samples = 0\n","  with torch.no_grad():\n","    for frame, _, label in test_loader:\n","      frame = frame.to(device)\n","      frame = frame.permute(2,0,1,3,4)  # [N, T, C, H, W] -> [T, N, C, H, W]\n","      label = label.to(device)\n","      label_onehot = F.one_hot(label, 51).float()\n","      out_fr = ann(frame)\n","      loss = F.mse_loss(out_fr, label_onehot)\n","      test_samples += label.numel()\n","      test_loss += loss.item() * label.numel()\n","      test_acc += (out_fr.argmax(1) == label).float().sum().item()\n","  test_time = time.time()\n","  test_speed = test_samples / (test_time - train_time)\n","  test_loss /= test_samples\n","  test_acc /= test_samples\n","  test_acc_list.append(test_acc)\n","\n","  if test_acc > max_test_acc:\n","    max_test_acc = test_acc\n","\n","\n","# weight 저장\n","torch.save(ann.state_dict(), \"ann_weights.pth\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":460},"id":"ykWi71qLnDAA","executionInfo":{"status":"error","timestamp":1741439982046,"user_tz":-540,"elapsed":119091,"user":{"displayName":"반도체시스템공학과/양소혜","userId":"15927555157164829597"}},"outputId":"4ddc39a2-5ecb-4a99-b5a8-6d09058d2fec"},"execution_count":null,"outputs":[{"output_type":"error","ename":"AttributeError","evalue":"module 'av' has no attribute 'AVError'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/io/video.py\u001b[0m in \u001b[0;36mread_video\u001b[0;34m(filename, start_pts, end_pts, pts_unit, output_format)\u001b[0m\n\u001b[1;32m    306\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 307\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0mav\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata_errors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcontainer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mcontainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstreams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maudio\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: ","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-10-a5218c3bd3dc>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0mtrain_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# [N,T,C,H,W] -> [T,N,C,H,W]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    699\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             if (\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    755\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 757\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    758\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    759\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"__getitems__\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m__getitems__\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m    418\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 420\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    418\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 420\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/datasets/hmdb51.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m         \u001b[0mvideo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maudio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvideo_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvideo_clips\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_clip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m         \u001b[0msample_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvideo_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msample_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/datasets/video_utils.py\u001b[0m in \u001b[0;36mget_clip\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    326\u001b[0m             \u001b[0mstart_pts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclip_pts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m             \u001b[0mend_pts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclip_pts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m             \u001b[0mvideo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maudio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_video\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_pts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_pts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    329\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[0m_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_probe_video_from_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/io/video.py\u001b[0m in \u001b[0;36mread_video\u001b[0;34m(filename, start_pts, end_pts, pts_unit, output_format)\u001b[0m\n\u001b[1;32m    333\u001b[0m                     \u001b[0minfo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"audio_fps\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstreams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maudio\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 335\u001b[0;31m         \u001b[0;32mexcept\u001b[0m \u001b[0mav\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAVError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    336\u001b[0m             \u001b[0;31m# TODO raise a warning?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m             \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: module 'av' has no attribute 'AVError'"]}]},{"cell_type":"markdown","source":["# SNN"],"metadata":{"id":"woyvAadn1xTQ"}},{"cell_type":"code","source":["class SNN(nn.Module):\n","  def __init__(self, input_size, output_size, tau=2.0):\n","    super(SNN, self).__init__()\n","    self.network = nn.Sequential(\n","      nn.Flatten(),\n","      nn.Linear(input_size, output_size),\n","      neuron.LIFNode(tau=tau)\n","    )\n","\n","  def forward(self, x):\n","    x = self.network(x)\n","    return x"],"metadata":{"id":"8D16-X8i2TC-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# weight 불러오기\n","ann_weights = torch.load(\"ann_weights.pth\", map_location=device)\n","\n","# snn weight으로 변환\n","max_weight = max(abs(w.max().item()) for w in ann_weights.value())\n","snn_weights = {k: v / max_weight for k, v in ann_weights.items()}\n","\n","# SNN 초기화 및 snn_weights 적용\n","snn = SNN(input_size, output_size).to(device)\n","snn.load_state_dict(snn_weights)"],"metadata":{"id":"_w766ijt8T_F"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# train\n","snn.eval()\n","test_loss = 0\n","test_acc = 0\n","test_samples = 0\n","\n","with torch.no_grad():\n","  for frame, _, label in test_loader:\n","    frame = frame.to(device)\n","    frame = frame.permute(2,0,1,3,4)  # [N, T, C, H, W] -> [T, N, C, H, W]\n","    label = label.to(device)\n","    label_onehot = F.one_hot(label, 51).float()\n","    out_fr = snn(frame)\n","    loss = F.mse_loss(out_fr, label_onehot)\n","    test_samples += label.numel()\n","    test_loss += loss.item() * label.numel()\n","    test_acc += (out_fr.argmax(1) == label).float().sum().item()\n","test_time = time.time()\n","test_speed = test_samples / (test_time - train_time)\n","test_loss /= test_samples\n","test_acc /= test_samples\n","test_acc_list.append(test_acc)"],"metadata":{"id":"r7sW8LEL2SjN"},"execution_count":null,"outputs":[]}]}